[package]
name = "zeph-llm"
version.workspace = true
edition.workspace = true
authors.workspace = true
license.workspace = true
repository.workspace = true

[features]
default = []
mock = []
candle = ["dep:candle-core", "dep:candle-nn", "dep:candle-transformers", "dep:hf-hub", "dep:tokenizers"]
compatible = ["openai"]
cuda = ["candle", "candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
metal = ["candle", "candle-core/metal", "candle-nn/metal", "candle-transformers/metal"]
openai = []
orchestrator = []
router = []

[dependencies]
thiserror.workspace = true
candle-core = { workspace = true, optional = true }
candle-nn = { workspace = true, optional = true }
candle-transformers = { workspace = true, optional = true }
eventsource-stream.workspace = true
futures-core.workspace = true
hf-hub = { workspace = true, optional = true }
ollama-rs.workspace = true
reqwest = { workspace = true, features = ["json", "rustls", "stream"] }
serde = { workspace = true, features = ["derive"] }
serde_json.workspace = true
tokenizers = { workspace = true, optional = true }
tokio = { workspace = true, features = ["rt", "sync", "time"] }
tokio-stream.workspace = true
tracing.workspace = true

[dev-dependencies]
anyhow.workspace = true
tokio = { workspace = true, features = ["macros", "rt-multi-thread"] }

[lints]
workspace = true
