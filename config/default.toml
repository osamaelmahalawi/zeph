[agent]
# Agent display name
name = "Zeph"
# Maximum tool execution iterations per user message (doom-loop protection)
max_tool_iterations = 10
# Optional local model for tool output summarization and context compaction.
# Format: "ollama/<model>". Falls back to primary provider if unset.
# summary_model = "ollama/llama3.2"

[llm]
# LLM provider: "ollama" for local models or "claude" for Claude API
provider = "ollama"
# Base URL for Ollama server
base_url = "http://localhost:11434"
# Primary model for chat completions
model = "mistral:7b"
# Model for generating embeddings (semantic memory)
embedding_model = "qwen3-embedding"

[llm.cloud]
# Claude API model (used when provider = "claude")
model = "claude-sonnet-4-5-20250929"
# Maximum tokens for Claude responses
max_tokens = 4096

# OpenAI-compatible API (GPT-5.2, Together, Groq, Fireworks, etc.)
# [llm.openai]
# base_url = "https://api.openai.com/v1"
# model = "gpt-5.2"
# max_tokens = 4096
# embedding_model = "text-embedding-3-small"
# reasoning_effort = "medium"  # low, medium, high (for reasoning models)

# Candle local inference (enabled by default, disable with --no-default-features)
# [llm.candle]
# source = "huggingface"  # "local" or "huggingface"
# repo_id = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
# filename = "mistral-7b-instruct-v0.2.Q4_K_M.gguf"
# local_path = ""          # used when source = "local"
# chat_template = "mistral"  # llama3, chatml, mistral, phi3, raw
# device = "auto"          # auto, cpu, metal, cuda
# embedding_repo = "sentence-transformers/all-MiniLM-L6-v2"
# [llm.candle.generation]
# temperature = 0.7
# top_p = 0.9
# top_k = 40
# max_tokens = 2048
# seed = 42
# repeat_penalty = 1.1

# Model orchestrator (enabled by default)
# Routes tasks to different providers with fallback chains
# [llm.orchestrator]
# default = "ollama"
# embed = "ollama"
# [llm.orchestrator.providers.ollama]
# provider_type = "ollama"
# [llm.orchestrator.providers.claude]
# provider_type = "claude"
# [llm.orchestrator.routes]
# coding = ["claude", "ollama"]
# creative = ["claude", "ollama"]
# general = ["ollama"]

[skills]
# Directories to scan for SKILL.md files
paths = ["./skills"]
# Maximum number of skills to inject into context per query (embedding-based selection)
max_active_skills = 5

[skills.learning]
# Enable self-learning skill improvement (feature enabled by default, runtime toggle)
enabled = false
# Automatically activate improved versions (false = require manual approval)
auto_activate = false
# Minimum failures before generating improvement
min_failures = 3
# Success rate threshold below which improvement is triggered (0.0-1.0)
improve_threshold = 0.7
# Success rate below which automatic rollback occurs (0.0-1.0)
rollback_threshold = 0.5
# Minimum evaluations before rollback decision
min_evaluations = 5
# Maximum auto-generated versions per skill
max_versions = 10
# Cooldown between improvements for same skill (minutes)
cooldown_minutes = 60

[memory]
# SQLite database path for conversation history
sqlite_path = "./data/zeph.db"
# Maximum number of recent messages to load into context
history_limit = 50
# Qdrant vector database URL for semantic memory
qdrant_url = "http://localhost:6334"
# Number of messages before triggering summarization (0 = disabled)
summarization_threshold = 50
# Total token budget for context window (0 = auto-detect from model)
context_budget_tokens = 0
# Auto-detect context budget from model's context window size
auto_budget = true
# Context compaction threshold (0.0-1.0): compact when usage exceeds this fraction
compaction_threshold = 0.80
# Number of recent messages to preserve during compaction
compaction_preserve_tail = 6
# Token budget protected from tool output pruning (recent context zone)
prune_protect_tokens = 40000
# Minimum relevance score for cross-session memory results (0.0-1.0)
cross_session_score_threshold = 0.35

[memory.semantic]
# Enable semantic memory with vector search
enabled = true
# Maximum number of semantically relevant messages to recall
recall_limit = 5
# Hybrid search weights (vector + FTS5 keyword). Must sum to 1.0.
vector_weight = 0.7
keyword_weight = 0.3

# Code RAG: AST-based code indexing and hybrid retrieval
# Requires Qdrant and tree-sitter grammars (feature "index", not enabled by default)
# Enable with: cargo build --features index
[index]
# Enable code indexing and retrieval (requires Qdrant)
enabled = false
# Watch for file changes and reindex incrementally
watch = true
# Maximum code chunks to retrieve per query
max_chunks = 12
# Minimum cosine similarity score to accept
score_threshold = 0.25
# Fraction of code_context budget used by retriever (0.0-1.0)
budget_ratio = 0.40
# Token budget for repo structural map in system prompt (0 = disabled)
repo_map_tokens = 500
# Cache TTL for repo map in seconds (avoids regeneration on every message)
repo_map_ttl_secs = 300

[mcp]
# Allowlist of permitted commands for /mcp add (empty = allow all)
allowed_commands = ["npx", "uvx", "node", "python", "python3"]
# Maximum number of dynamically added MCP servers
max_dynamic_servers = 10

# Stdio transport (spawn child process):
# [[mcp.servers]]
# id = "filesystem"
# command = "npx"
# args = ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"]
# timeout = 30

# HTTP transport (remote MCP server, e.g. Docker container):
# [[mcp.servers]]
# id = "remote-tools"
# url = "http://localhost:3001/mcp"
# timeout = 30

[vault]
# Secret retrieval backend: "env" reads from environment variables
backend = "env"

[a2a]
# Enable A2A server for agent-to-agent communication
enabled = false
# Bind address
host = "0.0.0.0"
# HTTP port
port = 8080
# Public URL advertised in AgentCard (auto-generated if empty)
public_url = ""
# Rate limit: max requests per minute per IP (0 = unlimited)
rate_limit = 60
# Require TLS for outbound A2A connections
require_tls = true
# Block requests to private/loopback IPs
ssrf_protection = true
# Maximum request body size in bytes (1MB)
max_body_size = 1048576

[tools]
# Enable tool execution (bash commands)
enabled = true
# Summarize long tool output via LLM instead of head+tail truncation
summarize_output = true

[tools.shell]
# Command timeout in seconds
timeout = 30
# Additional commands to block (case-insensitive, supports wildcards)
blocked_commands = []
# Commands to remove from the default blocklist (e.g., ["curl", "wget"])
allowed_commands = []
# Restrict file access to these paths (empty = current directory only)
allowed_paths = []
# Allow network commands (curl, wget, nc)
allow_network = true
# Commands that require user confirmation before execution
confirm_patterns = ["rm ", "git push -f", "git push --force", "drop table", "drop database", "truncate "]

[tools.scrape]
# HTTP request timeout in seconds
timeout = 15
# Maximum response body size in bytes (1MB)
max_body_bytes = 1048576

[tools.audit]
# Enable audit logging for tool executions
enabled = false
# Audit destination: "stdout" or file path (e.g., "./data/audit.jsonl")
destination = "stdout"

[security]
# Redact secrets (API keys, tokens) from LLM responses before display
redact_secrets = true
# Tool access level: "readonly" (observe only), "supervised" (default, with confirmations), "full" (all tools, no confirmations)
autonomy_level = "supervised"

# [telegram]
# token = "your-bot-token"
# Allowed usernames (empty = allow all except for /start command)
# allowed_users = ["username1", "username2"]

[timeouts]
# LLM chat completion timeout in seconds
llm_seconds = 120
# Embedding generation timeout in seconds
embedding_seconds = 30
# A2A remote call timeout in seconds
a2a_seconds = 30
