[agent]
# Agent display name
name = "Zeph"

[llm]
# LLM provider: "ollama" for local models or "claude" for Claude API
provider = "ollama"
# Base URL for Ollama server
base_url = "http://localhost:11434"
# Primary model for chat completions
model = "mistral:7b"
# Model for generating embeddings (semantic memory)
embedding_model = "qwen3-embedding"

[llm.cloud]
# Claude API model (used when provider = "claude")
model = "claude-sonnet-4-5-20250929"
# Maximum tokens for Claude responses
max_tokens = 4096

# Candle local inference (requires --features candle)
# [llm.candle]
# source = "huggingface"  # "local" or "huggingface"
# repo_id = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
# filename = "mistral-7b-instruct-v0.2.Q4_K_M.gguf"
# local_path = ""          # used when source = "local"
# chat_template = "mistral"  # llama3, chatml, mistral, phi3, raw
# device = "auto"          # auto, cpu, metal, cuda
# embedding_repo = "sentence-transformers/all-MiniLM-L6-v2"
# [llm.candle.generation]
# temperature = 0.7
# top_p = 0.9
# top_k = 40
# max_tokens = 2048
# seed = 42
# repeat_penalty = 1.1

# Model orchestrator (requires --features orchestrator)
# Routes tasks to different providers with fallback chains
# [llm.orchestrator]
# default = "ollama"
# embed = "ollama"
# [llm.orchestrator.providers.ollama]
# provider_type = "ollama"
# [llm.orchestrator.providers.claude]
# provider_type = "claude"
# [llm.orchestrator.routes]
# coding = ["claude", "ollama"]
# creative = ["claude", "ollama"]
# general = ["ollama"]

[skills]
# Directories to scan for SKILL.md files
paths = ["./skills"]
# Maximum number of skills to inject into context per query (embedding-based selection)
max_active_skills = 5

[skills.learning]
# Enable self-learning skill improvement (requires self-learning feature)
enabled = false
# Automatically activate improved versions (false = require manual approval)
auto_activate = false
# Minimum failures before generating improvement
min_failures = 3
# Success rate threshold below which improvement is triggered (0.0-1.0)
improve_threshold = 0.7
# Success rate below which automatic rollback occurs (0.0-1.0)
rollback_threshold = 0.5
# Minimum evaluations before rollback decision
min_evaluations = 5
# Maximum auto-generated versions per skill
max_versions = 10
# Cooldown between improvements for same skill (minutes)
cooldown_minutes = 60

[memory]
# SQLite database path for conversation history
sqlite_path = "./data/zeph.db"
# Maximum number of recent messages to load into context
history_limit = 50
# Qdrant vector database URL for semantic memory
qdrant_url = "http://localhost:6334"
# Number of messages before triggering summarization (0 = disabled)
summarization_threshold = 100
# Total token budget for context window (0 = unlimited)
context_budget_tokens = 0

[memory.semantic]
# Enable semantic memory with vector search
enabled = false
# Maximum number of semantically relevant messages to recall
recall_limit = 5

[vault]
# Secret retrieval backend: "env" reads from environment variables
backend = "env"

[a2a]
# Enable A2A server for agent-to-agent communication
enabled = false
# Bind address
host = "0.0.0.0"
# HTTP port
port = 8080
# Public URL advertised in AgentCard (auto-generated if empty)
public_url = ""
# Rate limit: max requests per minute per IP (0 = unlimited)
rate_limit = 60

[tools]
# Enable tool execution (bash commands)
enabled = true

[tools.shell]
# Command timeout in seconds
timeout = 30
# Additional commands to block (case-insensitive, supports wildcards)
blocked_commands = []
# Commands to remove from the default blocklist (e.g., ["curl", "wget"])
allowed_commands = []

[tools.scrape]
# HTTP request timeout in seconds
timeout = 15
# Maximum response body size in bytes (1MB)
max_body_bytes = 1048576
